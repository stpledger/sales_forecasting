{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093bf28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle/input\\holidays_events.csv\n",
      "kaggle/input\\oil.csv\n",
      "kaggle/input\\sample_submission.csv\n",
      "kaggle/input\\stores.csv\n",
      "kaggle/input\\test.csv\n",
      "kaggle/input\\train.csv\n",
      "kaggle/input\\transactions.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = 'kaggle/input'\n",
    "\n",
    "for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "377b6d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        date  store_nbr      family  sales  onpromotion\n",
       "0   0  2013-01-01          1  AUTOMOTIVE    0.0            0\n",
       "1   1  2013-01-01          1   BABY CARE    0.0            0\n",
       "2   2  2013-01-01          1      BEAUTY    0.0            0\n",
       "3   3  2013-01-01          1   BEVERAGES    0.0            0\n",
       "4   4  2013-01-01          1       BOOKS    0.0            0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d41b34b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Santo Domingo</td>\n",
       "      <td>Santo Domingo de los Tsachilas</td>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_nbr           city                           state type  cluster\n",
       "0          1          Quito                       Pichincha    D       13\n",
       "1          2          Quito                       Pichincha    D       13\n",
       "2          3          Quito                       Pichincha    D        8\n",
       "3          4          Quito                       Pichincha    D        9\n",
       "4          5  Santo Domingo  Santo Domingo de los Tsachilas    D        4"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores = pd.read_csv(os.path.join(path, 'stores.csv'))\n",
    "stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b5fcdb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_nbr: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54]\n",
      "length: 54\n",
      "\n",
      "family: ['AUTOMOTIVE' 'BABY CARE' 'BEAUTY' 'BEVERAGES' 'BOOKS' 'BREAD/BAKERY'\n",
      " 'CELEBRATION' 'CLEANING' 'DAIRY' 'DELI' 'EGGS' 'FROZEN FOODS' 'GROCERY I'\n",
      " 'GROCERY II' 'HARDWARE' 'HOME AND KITCHEN I' 'HOME AND KITCHEN II'\n",
      " 'HOME APPLIANCES' 'HOME CARE' 'LADIESWEAR' 'LAWN AND GARDEN' 'LINGERIE'\n",
      " 'LIQUOR,WINE,BEER' 'MAGAZINES' 'MEATS' 'PERSONAL CARE' 'PET SUPPLIES'\n",
      " 'PLAYERS AND ELECTRONICS' 'POULTRY' 'PREPARED FOODS' 'PRODUCE'\n",
      " 'SCHOOL AND OFFICE SUPPLIES' 'SEAFOOD']\n",
      "length: 33\n",
      "\n",
      "cluster: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'store_nbr: {np.sort(train_df['store_nbr'].unique())}\\nlength: {len(train_df['store_nbr'].unique())}\\n')\n",
    "\n",
    "print(f'family: {np.sort(train_df['family'].unique())}\\nlength: {len(train_df['family'].unique())}\\n')\n",
    "\n",
    "print(f'cluster: {np.sort(stores['cluster'].unique())}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f0890a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>92.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>93.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>93.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  dcoilwtico\n",
       "0 2013-01-01       93.14\n",
       "1 2013-01-02       93.14\n",
       "2 2013-01-03       92.97\n",
       "3 2013-01-04       93.12\n",
       "4 2013-01-07       93.20"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oil = pd.read_csv(os.path.join(path, 'oil.csv'))\n",
    "oil['date'] = pd.to_datetime(oil['date'])\n",
    "oil['dcoilwtico'] = oil['dcoilwtico'].ffill()\n",
    "oil['dcoilwtico'] = oil['dcoilwtico'].bfill()\n",
    "oil.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a2a18aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>locale</th>\n",
       "      <th>locale_name</th>\n",
       "      <th>description</th>\n",
       "      <th>transferred</th>\n",
       "      <th>holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-03-02</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Manta</td>\n",
       "      <td>Fundacion de Manta</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Regional</td>\n",
       "      <td>Cotopaxi</td>\n",
       "      <td>Provincializacion de Cotopaxi</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-04-12</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Cuenca</td>\n",
       "      <td>Fundacion de Cuenca</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-04-14</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Libertad</td>\n",
       "      <td>Cantonizacion de Libertad</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-04-21</td>\n",
       "      <td>Holiday</td>\n",
       "      <td>Local</td>\n",
       "      <td>Riobamba</td>\n",
       "      <td>Cantonizacion de Riobamba</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     type    locale locale_name                    description  \\\n",
       "0 2012-03-02  Holiday     Local       Manta             Fundacion de Manta   \n",
       "1 2012-04-01  Holiday  Regional    Cotopaxi  Provincializacion de Cotopaxi   \n",
       "2 2012-04-12  Holiday     Local      Cuenca            Fundacion de Cuenca   \n",
       "3 2012-04-14  Holiday     Local    Libertad      Cantonizacion de Libertad   \n",
       "4 2012-04-21  Holiday     Local    Riobamba      Cantonizacion de Riobamba   \n",
       "\n",
       "   transferred  holiday  \n",
       "0        False     True  \n",
       "1        False     True  \n",
       "2        False     True  \n",
       "3        False     True  \n",
       "4        False     True  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holiday_events = pd.read_csv(os.path.join(path, 'holidays_events.csv'))\n",
    "holiday_events['date'] = pd.to_datetime(holiday_events['date'])\n",
    "holiday_events = holiday_events[holiday_events['transferred'] == False]\n",
    "holiday_events['holiday'] = True\n",
    "holiday_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f760d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_nbr categories: Index([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "       19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
      "       37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54],\n",
      "      dtype='int64')\n",
      "family categories: Index(['AUTOMOTIVE', 'BABY CARE', 'BEAUTY', 'BEVERAGES', 'BOOKS',\n",
      "       'BREAD/BAKERY', 'CELEBRATION', 'CLEANING', 'DAIRY', 'DELI', 'EGGS',\n",
      "       'FROZEN FOODS', 'GROCERY I', 'GROCERY II', 'HARDWARE',\n",
      "       'HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES',\n",
      "       'HOME CARE', 'LADIESWEAR', 'LAWN AND GARDEN', 'LINGERIE',\n",
      "       'LIQUOR,WINE,BEER', 'MAGAZINES', 'MEATS', 'PERSONAL CARE',\n",
      "       'PET SUPPLIES', 'PLAYERS AND ELECTRONICS', 'POULTRY', 'PREPARED FOODS',\n",
      "       'PRODUCE', 'SCHOOL AND OFFICE SUPPLIES', 'SEAFOOD'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "store_nbr_cat = train_df['store_nbr'].astype('category').cat\n",
    "store_nbr_categories = store_nbr_cat.categories\n",
    "\n",
    "family_cat = train_df['family'].astype('category').cat\n",
    "family_categories = family_cat.categories\n",
    "\n",
    "print(f'store_nbr categories: {store_nbr_categories}')\n",
    "print(f'family categories: {family_categories}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4cc6733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def get_location(df):\n",
    "    df = df.merge(stores[['store_nbr', 'state', 'city']], on='store_nbr', how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_oil(df):\n",
    "    oil = pd.read_csv(os.path.join(path, 'oil.csv'))\n",
    "    oil['date'] = pd.to_datetime(oil['date'])\n",
    "    \n",
    "    df = df.merge(oil[['date', 'dcoilwtico']], on='date', how='left')\n",
    "    \n",
    "    df['dcoilwtico'] = df['dcoilwtico'].ffill()\n",
    "    df['dcoilwtico'] = df['dcoilwtico'].bfill()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_holiday(df):\n",
    "    holiday_events = pd.read_csv(os.path.join(path, 'holidays_events.csv'))\n",
    "    holiday_events['date'] = pd.to_datetime(holiday_events['date'])\n",
    "\n",
    "    holiday_events = holiday_events[holiday_events['transferred'] == False]\n",
    "\n",
    "    holidays = holiday_events[holiday_events['type'].isin(['Holiday', 'Transfer'])].copy()\n",
    "    holidays['holiday'] = True\n",
    "\n",
    "    special_days = holiday_events[holiday_events['type'].isin(['Additional', 'Event', 'Bridge'])].copy()\n",
    "    special_days['special_day'] = True\n",
    "\n",
    "    # Merge holidays\n",
    "    df = df.merge(holidays[holidays['locale'] == 'National'][['date', 'holiday']], on='date', how='left')\n",
    "    df = df.merge(holidays[holidays['locale'] == 'Regional'][['date', 'locale_name', 'holiday']], left_on=['date', 'state'], right_on=['date', 'locale_name'], how='left')\n",
    "    df = df.merge(holidays[holidays['locale'] == 'Local'][['date', 'locale_name', 'holiday']], left_on=['date', 'city'], right_on=['date', 'locale_name'], how='left')\n",
    "    \n",
    "    df['holiday'] = df['holiday'] | df['holiday_y'] | df['holiday_x']\n",
    "    df = df.drop(columns=['holiday_x', 'holiday_y', 'locale_name_x', 'locale_name_y'])\n",
    "    df['holiday'] = df['holiday'].fillna(0).astype(int)\n",
    "\n",
    "    df['before_holiday'] = df.groupby(['store_nbr'])['holiday'].shift(-1).fillna(0).astype(int)\n",
    "    df['after_holiday'] = df.groupby(['store_nbr'])['holiday'].shift(1).fillna(0).astype(int)\n",
    "\n",
    "    # Merge special days\n",
    "    df = df.merge(special_days[special_days['locale'] == 'National'][['date', 'special_day']], on='date', how='left')\n",
    "    df = df.merge(special_days[special_days['locale'] == 'Regional'][['date', 'locale_name', 'special_day']], left_on=['date', 'state'], right_on=['date', 'locale_name'], how='left')\n",
    "    df = df.merge(special_days[special_days['locale'] == 'Local'][['date', 'locale_name', 'special_day']], left_on=['date', 'city'], right_on=['date', 'locale_name'], how='left')\n",
    "    \n",
    "    df['special_day'] = df['special_day'] | df['special_day_y'] | df['special_day_x']\n",
    "    df = df.drop(columns=['special_day_x', 'special_day_y', 'locale_name_x', 'locale_name_y'])\n",
    "    df['special_day'] = df['special_day'].fillna(0).astype(int)\n",
    "\n",
    "    df['store_closed'] = ((df['date'].dt.month == 1) & (df['date'].dt.day == 1)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_seasonality(df, order=1, min_date=pd.to_datetime('2013-01-01'), max_date=pd.to_datetime('2017-08-31')):\n",
    "    uniq_idx = pd.date_range(min_date, max_date, freq='D')\n",
    "\n",
    "    fourier = CalendarFourier(freq='YE', order=order)\n",
    "\n",
    "    dp = DeterministicProcess(\n",
    "        index = uniq_idx,\n",
    "        order = 0,\n",
    "        constant = False,\n",
    "        seasonal = True,\n",
    "        additional_terms = [fourier],\n",
    "        drop = False\n",
    "    )\n",
    "\n",
    "    det_feats = dp.in_sample()\n",
    "\n",
    "    df = df.merge(det_feats, left_on='date', right_index=True, how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_lags(df, lags = [1]):\n",
    "    lag_names = [f'sales_lag_{lag}' for lag in lags]\n",
    "\n",
    "    for lag in lags:\n",
    "        df[f'sales_lag_{lag}'] = df.groupby(['store_nbr', 'family'])['sales'].shift(lag)\n",
    "        df[f'sales_lag_{lag}'] = df[f'sales_lag_{lag}'].fillna(0)\n",
    "\n",
    "    df['pct_change_cur'] = (\n",
    "        df\n",
    "        .groupby(['store_nbr', 'family'])['sales']\n",
    "        .transform(lambda s: s.pct_change(periods=1))\n",
    "    )\n",
    "    df['pct_change_cur'] = df['pct_change_cur'].fillna(1)\n",
    "\n",
    "    df['pct_change'] = df.groupby(['store_nbr', 'family'])['pct_change_cur'].shift(1)\n",
    "\n",
    "    df = df.drop(columns=['pct_change_cur'])\n",
    "    \n",
    "    return df, lag_names\n",
    "\n",
    "def get_windows(df, avg_windows=[7, 30], ):\n",
    "    window_names = [f'sales_window_{window}' for window in avg_windows]\n",
    "\n",
    "    for window in avg_windows:\n",
    "        df[f'sales_window_{window}'] = df.groupby(['store_nbr', 'family'])['sales'].transform(lambda x: x.rolling(window=window, min_periods=window).mean())\n",
    "        df[f'sales_window_{window}'] = df[f'sales_window_{window}'].fillna(0)\n",
    "\n",
    "    def slope(arr):\n",
    "        \"\"\"Return the linear trend (Δ per day) of the array.\"\"\"\n",
    "        x = np.arange(len(arr))\n",
    "        # polyfit on a 1-D array → coeffs[0] is slope\n",
    "        return np.polyfit(x, arr, 1)[0] / arr.mean()\n",
    "\n",
    "    # df['rolling_velocity'] = (\n",
    "    #     df\n",
    "    #     .groupby(['store_nbr', 'family'])['sales']\n",
    "    #     .transform(lambda s: (\n",
    "    #         s.rolling(window=7, min_periods=7)        # full 7-day window only\n",
    "    #         .apply(slope, raw=False)                 # raw=False → gets a Series\n",
    "\n",
    "    #         # windows that haven’t reached size 7 are NaN; keep or fill as you like\n",
    "    #     ))\n",
    "    # )\n",
    "\n",
    "    # df['rolling_velocity'] = df['rolling_velocity'].fillna(0)\n",
    "\n",
    "    # df['rolling_velocity'] = np.clip(df['rolling_velocity'], -1, 1)\n",
    "\n",
    "    return df, window_names\n",
    "\n",
    "\n",
    "def preprocess(df, proc_date=None, test=False):\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    df = get_location(df)\n",
    "    df = get_oil(df)\n",
    "    df = get_holiday(df)\n",
    "    df, lag_names = get_lags(df, lags=[1, 7])\n",
    "    df.loc[df[lag_names].eq(0).all(axis=1), 'sales'] = np.nan\n",
    "    df, window_names = get_windows(df, avg_windows=[7, 30])\n",
    "    df = get_seasonality(df, order=4)\n",
    "\n",
    "    df['store_nbr'] = df['store_nbr'].apply(lambda x: store_nbr_categories.get_loc(x))\n",
    "    df['family'] = df['family'].apply(lambda x: family_categories.get_loc(x))\n",
    "\n",
    "    if proc_date is not None:\n",
    "        df = df[df['date'] == proc_date]\n",
    "\n",
    "    if not test:\n",
    "        df = df[~(df[lag_names].eq(0).all(axis=1))]\n",
    "        df = df[~(df[window_names].eq(0).any(axis=1))]\n",
    "\n",
    "    for col in lag_names + window_names + ['sales']:\n",
    "        df[col] = np.log1p(df[col])\n",
    "\n",
    "    df['dcoilwtico'] = (df['dcoilwtico'] - 67.8197) / 25.6708\n",
    "\n",
    "    dates = df['date']\n",
    "    df = df.drop(columns=['date', 'state', 'city', 'id'])\n",
    "\n",
    "    return df, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "17c62053",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "train_sample = train_sample[(train_sample['store_nbr'] == 22) & (train_sample['family'] == 'HOME CARE')]\n",
    "train_sample = train_sample.reset_index(drop=True)\n",
    "train_sample, dates = preprocess(train_sample)\n",
    "\n",
    "train_sample['date'] = dates\n",
    "# train_sample.info()\n",
    "train_sample['day_of_week'] = train_sample['date'].dt.day_name()\n",
    "train_sample.to_csv('train_sample_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c197a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "train_sample = train_sample[(train_sample['store_nbr'] == 22) & (train_sample['family'] == 'HOME CARE')]\n",
    "train_sample['date'] = pd.to_datetime(train_sample['date'])\n",
    "train_sample = get_location(train_sample)\n",
    "holiday_test = get_holiday(train_sample)\n",
    "\n",
    "holiday_test.to_csv('holiday_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ca513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = preprocess(train_df)\n",
    "x = train_df.copy()\n",
    "x['date'] = pd.to_datetime(x['date'])\n",
    "x = x.merge(stores[['store_nbr', 'city', 'state']], on='store_nbr', how='left')\n",
    "x = get_oil(x)\n",
    "x = get_holiday(x)\n",
    "# x.sample(1000, random_state=42).to_csv('train_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665514e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = x[['store_nbr', 'family']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "combo_sample = x.merge(combos.sample(10, random_state=1), on=['store_nbr', 'family'], how='inner')\n",
    "\n",
    "combo_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfbf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_sample['date'] = pd.to_datetime(combo_sample['date'])\n",
    "combo_sample = combo_sample[combo_sample['date'].dt.year == 2016]\n",
    "combo_sample = combo_sample[combo_sample['date'].dt.month < 5]\n",
    "groups = combo_sample.groupby(['store_nbr', 'family'])\n",
    "\n",
    "fig, axs = plt.subplots(5, 2, figsize=(12, 15), sharex=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for ax, ((store_nbr, family), group) in zip(axs, groups):\n",
    "    group = group.sort_values('date')\n",
    "    ax.plot(group['date'], group['sales'], marker='o', markersize=2)\n",
    "    ax.set_title(f'Store {store_nbr}, Family: {family}')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.set_ylim(group['sales'].min(), group['sales'].max())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947d5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first group from the groups object and sort by date\n",
    "first_key, first_group = list(groups)[1]\n",
    "first_group = first_group.sort_values('date')\n",
    "sales = first_group['sales'].reset_index(drop=True)\n",
    "\n",
    "# Create lag plots for lags 1 through 7\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for lag in range(1, 8): \n",
    "    shifted = sales.shift(lag)\n",
    "    # Ensure we only compare valid (non-NaN) points\n",
    "    valid = shifted.notna()\n",
    "    axes[lag - 1].scatter(shifted[valid], sales[valid], s=20, alpha=0.7)\n",
    "    axes[lag - 1].set_title(f'Lag {lag}')\n",
    "    axes[lag - 1].set_xlabel(f'sales(t-{lag})')\n",
    "    axes[lag - 1].set_ylabel('sales(t)')\n",
    "\n",
    "# Disable the unused subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f90a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "plot_acf(sales, lags=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40761017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/code/ryanholbrook/seasonality\n",
    "\n",
    "def plot_periodogram(ts, detrend='linear', ax=None):\n",
    "    from scipy.signal import periodogram\n",
    "    fs = pd.Timedelta(\"365D\") / pd.Timedelta(\"1D\")\n",
    "    freqencies, spectrum = periodogram(\n",
    "        ts,\n",
    "        fs=fs,\n",
    "        detrend=detrend,\n",
    "        window=\"boxcar\",\n",
    "        scaling='spectrum',\n",
    "    )\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.step(freqencies, spectrum, color=\"purple\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n",
    "    ax.set_xticklabels(\n",
    "        [\n",
    "            \"Annual (1)\",\n",
    "            \"Semiannual (2)\",\n",
    "            \"Quarterly (4)\",\n",
    "            \"Bimonthly (6)\",\n",
    "            \"Monthly (12)\",\n",
    "            \"Biweekly (26)\",\n",
    "            \"Weekly (52)\",\n",
    "            \"Semiweekly (104)\",\n",
    "        ],\n",
    "        rotation=30,\n",
    "    )\n",
    "    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n",
    "    ax.set_ylabel(\"Variance\")\n",
    "    ax.set_title(\"Periodogram\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebcabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_periodogram(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635da7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_x = pd.read_csv(os.path.join(path, 'test.csv'))\n",
    "# test_x = preprocess(test_x, test_set=True)\n",
    "\n",
    "# test_x.to_csv('test_preprocessed.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, dates = preprocess(train_df)\n",
    "train_x.sample(10000, random_state=42).to_csv('train_preprocessed.csv', index=True)\n",
    "train_x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, target_col='sales'):\n",
    "        self.dataframe = dataframe\n",
    "        self.target_col = target_col\n",
    "        self.features = dataframe.drop(columns=[target_col])\n",
    "        self.targets = dataframe[target_col].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vals = self.features.iloc[idx].values\n",
    "        store_nbr = torch.tensor(vals[0], dtype=torch.long)\n",
    "        family = torch.tensor(vals[1], dtype=torch.long)\n",
    "        x_cont = torch.tensor(vals[2:6], dtype=torch.float32)\n",
    "        x_time = torch.tensor(vals[6:], dtype=torch.float32)\n",
    "        y = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        return store_nbr, family, x_cont, x_time, y\n",
    "    \n",
    "def load_data(df, ttsplit=False, **kwargs):\n",
    "    df, _ = preprocess(df)\n",
    "    if ttsplit:\n",
    "        train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        train_dataset = CustomDataset(train_df)\n",
    "        val_dataset = CustomDataset(val_df)\n",
    "        return DataLoader(train_dataset, **kwargs), DataLoader(val_dataset, **kwargs)\n",
    "    dataset = CustomDataset(df)\n",
    "    return DataLoader(dataset, **kwargs)\n",
    "    \n",
    "class RMSLELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        log_y_pred = torch.log1p(y_pred)\n",
    "        log_y_true = torch.log1p(y_true)\n",
    "        loss = torch.mean((log_y_pred - log_y_true) ** 2)\n",
    "        return torch.sqrt(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479367ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Standardize(torch.nn.Module):\n",
    "#     def __init__(self, mean, std):\n",
    "#         super().__init__()\n",
    "#         self.register_buffer(\"mean\", torch.tensor(mean, dtype=torch.float32))\n",
    "#         self.register_buffer(\"std\",  torch.tensor(std,  dtype=torch.float32))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return (x - self.mean) / self.std\n",
    "\n",
    "class SalesModel(torch.nn.Module):\n",
    "    def __init__(self, num_stores, store_embed, num_families, fam_embed, input_size):\n",
    "        super().__init__()\n",
    "        self.store_embedding = torch.nn.Embedding(num_stores, store_embed)\n",
    "        self.family_embedding = torch.nn.Embedding(num_families, fam_embed)\n",
    "        # self.standardize = Standardize(scaler.mean_.astype(np.float32), scaler.scale_.astype(np.float32))\n",
    "        self.fc1 = torch.nn.Linear(store_embed + fam_embed + input_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, store_nbr, family, x_cont, x_time):\n",
    "        store_emb = self.store_embedding(store_nbr)\n",
    "        family_emb = self.family_embedding(family)\n",
    "        # x_stand = self.standardize(x_cont)\n",
    "        x = torch.cat([store_emb, family_emb, x_cont, x_time], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "\n",
    "def train(\n",
    "    exp_dir: str = \"logs\",\n",
    "    num_epoch: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    batch_size: int = 128,\n",
    "    seed: int = 42,\n",
    "    **kwargs,\n",
    "):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model_name = 'sales_model'\n",
    "\n",
    "    # set random seed so each run is deterministic\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # directory with timestamp to save tensorboard logs and model checkpoints\n",
    "    log_dir = Path(exp_dir) / f\"{model_name}_{datetime.now().strftime('%m%d_%H%M%S')}\"\n",
    "    logger = SummaryWriter(log_dir)\n",
    "\n",
    "    model = SalesModel(\n",
    "        num_stores=54,\n",
    "        store_embed=4,\n",
    "        num_families=33,\n",
    "        fam_embed=4,\n",
    "        input_size=20,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    df = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "    train_data, val_data = load_data(df, ttsplit=True, shuffle=True, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # create loss function and optimizer\n",
    "    loss_func = MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    print('Training started...\\n')\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(num_epoch):\n",
    "        metrics = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "        for store_nbr, family, x_cont, x_time, y in train_data:\n",
    "            store_nbr, family, x_cont, x_time = store_nbr.to(device), family.to(device), x_cont.to(device), x_time.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            pred = model(store_nbr, family, x_cont, x_time)\n",
    "            loss = loss_func(pred, y)\n",
    "            metrics[\"train_loss\"].append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step % 100 == 0:\n",
    "                train_loss = torch.as_tensor(metrics[\"train_loss\"]).mean()\n",
    "                logger.add_scalar(\"train_loss\", train_loss, global_step)\n",
    "                if global_step % 5000 == 0:\n",
    "                    print(f\"Epoch: {epoch}, Step {global_step}: train_loss={train_loss:.4f}\")\n",
    "                metrics[\"train_loss\"] = []\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for store_nbr, family, x_cont, x_time, y in val_data:\n",
    "                store_nbr, family, x_cont, x_time = store_nbr.to(device), family.to(device), x_cont.to(device), x_time.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                pred = model(store_nbr, family, x_cont, x_time)\n",
    "                loss = loss_func(pred, y)\n",
    "                metrics['val_loss'].append(loss.item())\n",
    "\n",
    "        epoch_train_loss = torch.as_tensor(metrics[\"train_loss\"]).mean()\n",
    "        epoch_val_loss = torch.as_tensor(metrics[\"val_loss\"]).mean()\n",
    "\n",
    "        logger.add_scalar(\"train_loss\", epoch_train_loss, global_step)\n",
    "        logger.add_scalar(\"val_loss\", epoch_val_loss, global_step)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1:2d} / {num_epoch:2d}: \"\n",
    "            f\"train_loss={epoch_train_loss:.4f} \"\n",
    "            f\"val_loss={epoch_val_loss:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "    # save and overwrite the model in the root directory\n",
    "    torch.save(model.state_dict(), f'{model_name}.th')\n",
    "\n",
    "    # save a copy of model weights in the log directory\n",
    "    torch.save(model.state_dict(), log_dir / f'{model_name}.th')\n",
    "    \n",
    "    print(f\"Model saved to {log_dir / f'{model_name}.th'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model_path: str = \"sales_model.th\",\n",
    "    batch_size: int = 128,\n",
    "    **kwargs,\n",
    "):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"CUDA not available, using CPU\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model = SalesModel(\n",
    "        num_stores=54,\n",
    "        store_embed=4,\n",
    "        num_families=33,\n",
    "        fam_embed=4,\n",
    "        input_size=20,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    train_df = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(path, 'test.csv'))\n",
    "    test_df['sales'] = np.nan\n",
    "\n",
    "    merged_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "\n",
    "    proc_date = merged_df[merged_df['sales'].isna()]['date'].min()\n",
    "    context_date = proc_date - pd.Timedelta(days=8)\n",
    "    merged_df = merged_df[merged_df['date'] >= context_date]\n",
    "\n",
    "    print(f'Context date: {context_date}')\n",
    "    print(f'Processing date: {proc_date}')    \n",
    "\n",
    "    df, _ = preprocess(merged_df, proc_date, test=True)\n",
    "    test_data = DataLoader(CustomDataset(df, target_col='sales'), batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for store_nbr, family, x_cont, x_time, _ in test_data:\n",
    "            store_nbr, family, x_cont, x_time = store_nbr.to(device), family.to(device), x_cont.to(device), x_time.to(device)\n",
    "            pred = model(store_nbr, family, x_cont, x_time)\n",
    "            pred = torch.expm1(pred)\n",
    "            predictions.append(pred.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    merged_df.loc[merged_df['date'] == proc_date, 'sales'] = predictions\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aabaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'sales_model.th'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = SalesModel(\n",
    "    num_stores=54,\n",
    "    store_embed=4,\n",
    "    num_families=33,\n",
    "    fam_embed=4,\n",
    "    input_size=20,\n",
    ")\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(path, 'test.csv'))\n",
    "test_df['sales'] = np.nan\n",
    "\n",
    "merged_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "\n",
    "start_date = merged_df[merged_df['sales'].isna()]['date'].min()\n",
    "end_date = merged_df['date'].max()\n",
    "context_date = start_date - pd.Timedelta(days=31)\n",
    "merged_df = merged_df[merged_df['date'] >= context_date]\n",
    "\n",
    "print(f'Context date: {context_date}')\n",
    "\n",
    "for proc_date in pd.date_range(start_date, end_date):\n",
    "    print(f'Processing date: {proc_date}')\n",
    "\n",
    "    df, _ = preprocess(merged_df, proc_date, test=True)\n",
    "    # df = preprocess(merged_df, test=True)\n",
    "    # df.to_excel('eval_processed.xlsx', index=False)\n",
    "    # break\n",
    "    test_data = DataLoader(CustomDataset(df, target_col='sales'), batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for store_nbr, family, x_cont, x_time, _ in test_data:\n",
    "            store_nbr, family, x_cont, x_time = store_nbr.to(device), family.to(device), x_cont.to(device), x_time.to(device)\n",
    "            pred = model(store_nbr, family, x_cont, x_time)\n",
    "            pred = torch.expm1(pred)\n",
    "            predictions.append(pred.cpu().numpy())\n",
    "\n",
    "    predictions = np.concatenate(predictions)\n",
    "    predictions = np.clip(predictions, 0, None)\n",
    "    merged_df.loc[merged_df['date'] == proc_date, 'sales'] = predictions\n",
    "\n",
    "merged_df.to_excel('evaluation.xlsx', index=False)\n",
    "\n",
    "submission = merged_df[merged_df['date'] > pd.to_datetime('2017-08-15')][['id', 'sales']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f09cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "z, _ = preprocess(train_df)\n",
    "\n",
    "z = z[z['store_nbr'] == 0]\n",
    "z = z[z['family'] == 30]\n",
    "\n",
    "z.to_excel('eval_counter.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(path, 'test.csv'))\n",
    "test_df['sales'] = np.nan\n",
    "\n",
    "merged_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "\n",
    "proc_date = merged_df[merged_df['sales'].isna()]['date'].min()\n",
    "print(f'Processing date: {proc_date}')\n",
    "context_date = proc_date - pd.Timedelta(days=10)\n",
    "print(f'Context date: {context_date}')\n",
    "\n",
    "merged_df = merged_df[merged_df['date'] >= context_date]\n",
    "\n",
    "df = preprocess(merged_df, proc_date, test=True)\n",
    "\n",
    "merged_df.to_excel('merged_df.xlsx', index=False, sheet_name='merged_preprocessed')\n",
    "df.to_excel('eval_df.xlsx', index=False, sheet_name='df_preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa25462",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = \"logs\"\n",
    "num_epoch = 10\n",
    "lr = 1e-3\n",
    "batch_size = 256\n",
    "seed = 42\n",
    "\n",
    "train(\n",
    "    exp_dir=exp_dir,\n",
    "    num_epoch=num_epoch,\n",
    "    lr=lr,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
